{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-20T10:35:09.908445Z",
     "start_time": "2025-06-20T10:35:01.712796Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from dataset import Flickr8kDataset, Flickr8kPreprocessor\n",
    "from models import ClipCapModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T10:35:15.597699Z",
     "start_time": "2025-06-20T10:35:10.006304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = './data'\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'Images')\n",
    "CAPTIONS_FILE = os.path.join(DATA_DIR, 'captions.txt')\n",
    "\n",
    "preprocessor = Flickr8kPreprocessor(data_dir=DATA_DIR)\n",
    "\n",
    "captions_data = preprocessor.load_captions(captions_file='captions.txt')\n",
    "print(f'Loaded {len(captions_data)} images with captions.')\n",
    "\n",
    "# CLIP features\n",
    "features_path = os.path.join(DATA_DIR, 'clip_features.pkl')\n",
    "if not os.path.exists(features_path):\n",
    "    features_data = preprocessor.extract_clip_features(images_dir='Images', save_path='clip_features.pkl')\n",
    "else:\n",
    "    with open(features_path, 'rb') as f:\n",
    "        features_data = pickle.load(f)\n",
    "    print(f'Loaded features for {len(features_data[\"image_names\"])} images.')\n",
    "\n",
    "splits = preprocessor.prepare_train_data(features_data, captions_data, test_size=0.2, val_size=0.1)\n",
    "\n",
    "print(f\"Train: {len(splits['train'][0])} samples\")\n",
    "print(f\"Val: {len(splits['val'][0])} samples\")\n",
    "print(f\"Test: {len(splits['test'][0])} samples\")"
   ],
   "id": "25adbe9ceb078331",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8091 images with captions.\n",
      "Loaded features for 8091 images.\n",
      "Train: 28318 samples\n",
      "Val: 4046 samples\n",
      "Test: 8091 samples\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T10:35:21.284362Z",
     "start_time": "2025-06-20T10:35:15.612989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 2. Create Dataset and DataLoader\n",
    "\n",
    "# Hyperparameters\n",
    "PREFIX_LENGTH = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 10\n",
    "CLIP_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "GPT2_MODEL_NAME = \"openai-community/gpt2\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(GPT2_MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Flickr8kDataset(\n",
    "    image_features=splits['train'][0],\n",
    "    captions=splits['train'][1],\n",
    "    tokenizer=tokenizer,\n",
    "    prefix_length=PREFIX_LENGTH\n",
    ")\n",
    "\n",
    "val_dataset = Flickr8kDataset(\n",
    "    image_features=splits['val'][0],\n",
    "    captions=splits['val'][1],\n",
    "    tokenizer=tokenizer,\n",
    "    prefix_length=PREFIX_LENGTH\n",
    ")\n",
    "\n",
    "test_dataset = Flickr8kDataset(\n",
    "    image_features=splits['test'][0],\n",
    "    captions=splits['test'][1],\n",
    "    tokenizer=tokenizer,\n",
    "    prefix_length=PREFIX_LENGTH\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f'Train batches: {len(train_loader)}')\n",
    "print(f'Val batches: {len(val_loader)}')\n",
    "print(f'Test batches: {len(test_loader)}')\n"
   ],
   "id": "fbfc1dde4e4ee693",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 885\n",
      "Val batches: 127\n",
      "Test batches: 253\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T10:35:23.020944Z",
     "start_time": "2025-06-20T10:35:21.298357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 3. Initialize Model\n",
    "\n",
    "# Initialize model\n",
    "model = ClipCapModel(\n",
    "    prefix_length=PREFIX_LENGTH,\n",
    ").to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS * len(train_loader))\n",
    "\n",
    "print(f'{sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.')\n"
   ],
   "id": "a1793efdf29d2ddf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137,031,936 trainable parameters.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T10:35:23.043051Z",
     "start_time": "2025-06-20T10:35:23.036349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 4. Training Loop\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device, tokenizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        image_features = batch['image_features'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Labels are the same as input_ids, with padding ignored\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = model(\n",
    "            image_features=image_features,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, device, tokenizer):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc='Validation')\n",
    "        for batch in progress_bar:\n",
    "            image_features = batch['image_features'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            labels = input_ids.clone()\n",
    "            labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "            outputs = model(\n",
    "                image_features=image_features,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ],
   "id": "9e396c7faf2e60b8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:46:38.600798Z",
     "start_time": "2025-06-20T11:17:27.393320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\nEpoch {epoch+1}/{EPOCHS}')\n",
    "    print('-' * 50)\n",
    "\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device, tokenizer)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    val_loss = validate_epoch(model, val_loader, device, tokenizer)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, 'best_model.pth')\n",
    "        print('Saved best model!')\n",
    "\n",
    "print('Training completed!')\n"
   ],
   "id": "bb4678a14324d78f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/885 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Training:  40%|████      | 358/885 [29:10<42:56,  4.89s/it, loss=1.82, lr=4.98e-5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mEPOCHS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m50\u001B[39m)\n\u001B[1;32m----> 9\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m train_epoch(model, train_loader, optimizer, scheduler, device, tokenizer)\n\u001B[0;32m     10\u001B[0m train_losses\u001B[38;5;241m.\u001B[39mappend(train_loss)\n\u001B[0;32m     12\u001B[0m val_loss \u001B[38;5;241m=\u001B[39m validate_epoch(model, val_loader, device, tokenizer)\n",
      "Cell \u001B[1;32mIn[5], line 31\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, dataloader, optimizer, scheduler, device, tokenizer)\u001B[0m\n\u001B[0;32m     28\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     29\u001B[0m     scheduler\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 31\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     32\u001B[0m     progress_bar\u001B[38;5;241m.\u001B[39mset_postfix({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m: loss\u001B[38;5;241m.\u001B[39mitem(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m: scheduler\u001B[38;5;241m.\u001B[39mget_last_lr()[\u001B[38;5;241m0\u001B[39m]})\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m total_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataloader)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## 5. Visualization and Evaluation\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "9a73006bbc6e3f44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:49:10.505764Z",
     "start_time": "2025-06-20T11:48:47.914561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the best model\n",
    "checkpoint = torch.load('best_model.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "# Generate sample captions\n",
    "def generate_sample_captions(model, dataloader, tokenizer, device, num_samples=5):\n",
    "    model.eval()\n",
    "    samples = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if len(samples) >= num_samples:\n",
    "                break\n",
    "\n",
    "            image_features = batch['image_features'].to(device)\n",
    "            input_ids = batch['input_ids']\n",
    "\n",
    "            generated_captions = model.generate(\n",
    "                image_features,\n",
    "                max_length=50,\n",
    "                temperature=0.8,\n",
    "                do_sample=True,\n",
    "                top_p=0.9\n",
    "            )\n",
    "\n",
    "            for i in range(len(generated_captions)):\n",
    "                if len(samples) >= num_samples:\n",
    "                    break\n",
    "                gt_caption = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
    "                samples.append({\n",
    "                    'generated': generated_captions[i],\n",
    "                    'ground_truth': gt_caption\n",
    "                })\n",
    "    return samples\n",
    "\n",
    "# Generate and display sample captions from the test set\n",
    "sample_captions = generate_sample_captions(model, test_loader, tokenizer, device, num_samples=10)\n",
    "\n",
    "for i, sample in enumerate(sample_captions):\n",
    "    print(f'\\n--- Sample {i+1} ---')\n",
    "    print(f'Generated:    {sample[\"generated\"]}')\n",
    "    print(f'Ground Truth: {sample[\"ground_truth\"]}')\n"
   ],
   "id": "90d83f5df73022d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_25900\\3638255213.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_model.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample 1 ---\n",
      "Generated:    A lioness is chasing after a black animal .\n",
      "Ground Truth: A large wild cat is pursuing a horse across a meadow .\n",
      "\n",
      "--- Sample 2 ---\n",
      "Generated:    Two dogs are fighting over a stick .\n",
      "Ground Truth: Two brown dogs fight on the leafy ground .\n",
      "\n",
      "--- Sample 3 ---\n",
      "Generated:    A man in a white shirt is standing on a cliff overlooking the ocean .\n",
      "Ground Truth: A man in shorts is standing on a rock looking out at the view from the hilltop .\n",
      "\n",
      "--- Sample 4 ---\n",
      "Generated:    A white dog with a muzzle runs through the grass with a stick in its mouth .\n",
      "Ground Truth: a muzzled white dog is running on the grass .\n",
      "\n",
      "--- Sample 5 ---\n",
      "Generated:    A skier in a red jacket is skiing down a snowy hill .\n",
      "Ground Truth: A person skiing downhill .\n",
      "\n",
      "--- Sample 6 ---\n",
      "Generated:    A German shepherd dog is running through the grass with a tennis ball in its mouth .\n",
      "Ground Truth: Shepherd dog catches tennis ball\n",
      "\n",
      "--- Sample 7 ---\n",
      "Generated:    A man in a yellow shirt is jumping on a trampoline .\n",
      "Ground Truth: A skateboarder is riding on a red ramp by the ocean .\n",
      "\n",
      "--- Sample 8 ---\n",
      "Generated:    A group of men playing cricket .\n",
      "Ground Truth: Two men dressed in white hit a ball while a third man walks up .\n",
      "\n",
      "--- Sample 9 ---\n",
      "Generated:    Three men are standing in front of a display case .\n",
      "Ground Truth: Three men sit at the counter in a restaurant .\n",
      "\n",
      "--- Sample 10 ---\n",
      "Generated:    A large crowd of people dressed in colorful costumes are gathered in front of a statue of a man in a red hat .\n",
      "Ground Truth: There is a large gathering of people in costumes outside on the street .\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "64a9ce0e10a0c77e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
